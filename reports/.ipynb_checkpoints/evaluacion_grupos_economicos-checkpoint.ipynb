{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8ce9e2-feee-4e39-9d13-71b8c11242d5",
   "metadata": {},
   "source": [
    "### Comparacion con grupos economicos reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "172dca0e-9d25-491d-97f0-f5b8f1593206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "from pyspark_dist_explore import hist\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.types import StringType,TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b83dc1-0580-4da6-bed3-9fdaa721b36c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 15:20:32 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "24/03/19 15:20:32 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "Setting spark.hadoop.yarn.resourcemanager.principal to hvega.externo\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/19 15:20:32 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "24/03/19 15:20:32 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "24/03/19 15:20:32 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "24/03/19 15:20:33 WARN Utils: Service 'SparkUI' could not bind on port 20049. Attempting port 20050.\n",
      "24/03/19 15:20:36 WARN HadoopFSDelegationTokenProvider: Token ABFS/IDBroker has not set up issue date properly. (provided: 0) Using current timestamp (1710861636751) as issue date instead. Consult token implementor to fix the behavior.\n",
      "24/03/19 15:20:36 WARN HadoopFSDelegationTokenProvider: Token ABFS/IDBroker has not set up issue date properly. (provided: 0) Using current timestamp (1710861636755) as issue date instead. Consult token implementor to fix the behavior.\n",
      "24/03/19 15:20:37 WARN HiveServer2CredentialProvider: Failed to get HS2 delegation token\n",
      "java.util.NoSuchElementException: spark.sql.hive.hiveserver2.jdbc.url\n",
      "\tat org.apache.spark.SparkConf.$anonfun$get$1(SparkConf.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.SparkConf.get(SparkConf.scala:245)\n",
      "\tat com.hortonworks.spark.deploy.yarn.security.HiveServer2CredentialProvider.obtainDelegationTokens(HiveServer2CredentialProvider.scala:64)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.$anonfun$obtainDelegationTokens$2(HadoopDelegationTokenManager.scala:164)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:213)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$obtainDelegationTokens(HadoopDelegationTokenManager.scala:162)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:226)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:224)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.obtainTokensAndScheduleRenewal(HadoopDelegationTokenManager.scala:224)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$updateTokensTask(HadoopDelegationTokenManager.scala:198)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.start(HadoopDelegationTokenManager.scala:123)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1(CoarseGrainedSchedulerBackend.scala:552)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1$adapted(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.start(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.start(KubernetesClusterSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:581)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/03/19 15:20:38 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "  .appName(\"Test\")  \\\n",
    "  .config(\"spark.yarn.access.hadoopFileSystems\",\"abfs://data@datalakesii.dfs.core.windows.net/\") \\\n",
    "  .config(\"spark.executor.memory\", \"24g\") \\\n",
    "  .config(\"spark.driver.memory\", \"12g\")\\\n",
    "  .config(\"spark.executor.cores\", \"12\") \\\n",
    "  .config(\"spark.executor.instances\", \"24\") \\\n",
    "  .config(\"spark.driver.maxResultSize\", \"12g\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "sc=spark.sparkContext\n",
    "sc.setLogLevel ('ERROR')\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d4241d-1979-474d-9c93-5006fc09bcda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"abfs://data@datalakesii.dfs.core.windows.net/DatosOrigen/LibSDF/GE_APIUX_ARFI_E\").createOrReplaceTempView(\"grupos_conocidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92f61ea0-acd4-4ec1-9e5c-51eb2503fc0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PARU_RUT_E', 'COM']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select * from grupos_conocidos').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5af6879a-f56a-4282-9f2e-1ccdbb3f6689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "contaminados = spark.read.options(header=True,inferSchema=True,delimiter=\",\").csv(\"/home/cdsw/data/processed/comunidades_louvain/comunidades_resolution_study_louvain.csv\")\n",
    "contaminados = contaminados.withColumnRenamed(\"comunidad_0.8\", \"comunidad_08\")\n",
    "contaminados.createOrReplaceTempView(\"contaminados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e031dcb6-e9bd-4f6a-9c40-2d0c273f25ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------+----+\n",
      "|            cont_rut|             score|comunidad_08| COM|\n",
      "+--------------------+------------------+------------+----+\n",
      "|++2mOmlMTMf8FP8AS...|0.0245276619163978|           2|null|\n",
      "|++KCc/MVDP3ZynZeM...|0.0238121494634209|          14|null|\n",
      "|+/+IMFryGSJkidvlc...|0.0459751720687882|           4|null|\n",
      "|+/GP2pgvQ+KWPs5dT...|0.0232765130868591|          16|null|\n",
      "|+/vREtWUlsd1b34Xn...|0.0759831339341641|           1|null|\n",
      "|+03yIiPxc4t3f47ON...|0.0602053273478164|           1|null|\n",
      "|+05Zfb0fREv7TebIf...|0.0239793742431574|           9|null|\n",
      "|+0FIA8uSEJsh7AYNz...|0.0194387436436004|          14|null|\n",
      "|+0LkoCydq856ZyJDD...|0.1106377118166478|         206|null|\n",
      "|+0OjQciCWcSfdOt0I...|0.0198019731689134|          60|null|\n",
      "|+0YLOVpVfeciOQHQS...| 0.014667630105706|          75|null|\n",
      "|+0a2/CGulxi1jtj4B...|0.0175274910436338|          72|null|\n",
      "|+0hnbJjWfyzpcvwiv...|0.0594577734576783|          48|null|\n",
      "|+0jNsLdkYFhcEBWbg...|0.0144322074058573|          45|null|\n",
      "|+0qDT5kksuYUaxHfL...|0.0239133423275352|          14|null|\n",
      "|+0vz0fAeWHqQjT0qd...|0.0244090086219896|         189|null|\n",
      "|+1+pBbsRuZRa6iPux...|0.1425850199989881|         158|null|\n",
      "|+1172XXOGosgAOjad...|0.0219141057396394|           3|null|\n",
      "|+1DDRYdwvVgFFhJZF...|0.0169756642904163|           1|null|\n",
      "|+1UYoO7EggeX9CtjH...|0.0226288007883557|           1|null|\n",
      "+--------------------+------------------+------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select cont_rut, score, comunidad_08, COM from contaminados left join grupos_conocidos on contaminados.cont_rut=grupos_conocidos.PARU_RUT_E').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45726c4-bba6-489f-bce3-887725e978d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
